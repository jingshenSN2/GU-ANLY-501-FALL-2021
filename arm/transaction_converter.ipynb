{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Convert YouTube Comments into Transaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-19 19:19:05,850\tINFO worker.py:836 -- Calling ray.init() again after it has already been called.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "import ray\n",
    "import psutil\n",
    "import modin.pandas as pd\n",
    "from tqdm import tqdm\n",
    "from modin.config import ProgressBar\n",
    "\n",
    "ProgressBar.enable()\n",
    "num_cpus = psutil.cpu_count(logical=False)\n",
    "ray.init(num_cpus=num_cpus, ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Estimated completion of line 2:   0%           Elapsed time: 00:00, estimated remaining time: ?",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "def571d7f8834a05bd37a8f3016a9815"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "   LABEL     video_id                                               text  \\\n0      0  -2Rd0A_WTDQ   which makes the attacker have to go through a...   \n1      0  -2Rd0A_WTDQ                  the neck beard i can't look away    \n2      2  -2Rd0A_WTDQ   billion dollars is pathetic for a modern chip...   \n3     -1  -2Rd0A_WTDQ   imagine people fighting against facebook spyi...   \n4      4  -2Rd0A_WTDQ   if facebook was 1 a month it'd change everyth...   \n\n   like published_at published_week  \n0     0   2021-05-21     2021-05-17  \n1     0   2021-05-15     2021-05-10  \n2     0   2021-05-15     2021-05-10  \n3     0   2021-05-11     2021-05-10  \n4     0   2021-05-11     2021-05-10  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LABEL</th>\n      <th>video_id</th>\n      <th>text</th>\n      <th>like</th>\n      <th>published_at</th>\n      <th>published_week</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-2Rd0A_WTDQ</td>\n      <td>which makes the attacker have to go through a...</td>\n      <td>0</td>\n      <td>2021-05-21</td>\n      <td>2021-05-17</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>-2Rd0A_WTDQ</td>\n      <td>the neck beard i can't look away</td>\n      <td>0</td>\n      <td>2021-05-15</td>\n      <td>2021-05-10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>-2Rd0A_WTDQ</td>\n      <td>billion dollars is pathetic for a modern chip...</td>\n      <td>0</td>\n      <td>2021-05-15</td>\n      <td>2021-05-10</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1</td>\n      <td>-2Rd0A_WTDQ</td>\n      <td>imagine people fighting against facebook spyi...</td>\n      <td>0</td>\n      <td>2021-05-11</td>\n      <td>2021-05-10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>-2Rd0A_WTDQ</td>\n      <td>if facebook was 1 a month it'd change everyth...</td>\n      <td>0</td>\n      <td>2021-05-11</td>\n      <td>2021-05-10</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/comments_labeled.csv')\n",
    "df = df[df['text'].str.len() > 0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load spacy pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# python -m spacy download en_core_web_sm\n",
    "pipeline = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process one comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ' ', 'lemma': ' ', 'POS': 'SPACE', 'tag': '_SP', 'dep': 'compound', 'shape': ' ', 'is_alpha': False, 'is_stop': False}\n",
      "{'text': 'billion', 'lemma': 'billion', 'POS': 'NUM', 'tag': 'CD', 'dep': 'nummod', 'shape': 'xxxx', 'is_alpha': True, 'is_stop': False}\n",
      "{'text': 'dollars', 'lemma': 'dollar', 'POS': 'NOUN', 'tag': 'NNS', 'dep': 'nsubj', 'shape': 'xxxx', 'is_alpha': True, 'is_stop': False}\n",
      "{'text': 'is', 'lemma': 'be', 'POS': 'AUX', 'tag': 'VBZ', 'dep': 'ROOT', 'shape': 'xx', 'is_alpha': True, 'is_stop': True}\n",
      "{'text': 'pathetic', 'lemma': 'pathetic', 'POS': 'ADJ', 'tag': 'JJ', 'dep': 'acomp', 'shape': 'xxxx', 'is_alpha': True, 'is_stop': False}\n",
      "{'text': 'for', 'lemma': 'for', 'POS': 'ADP', 'tag': 'IN', 'dep': 'mark', 'shape': 'xxx', 'is_alpha': True, 'is_stop': True}\n",
      "{'text': 'a', 'lemma': 'a', 'POS': 'DET', 'tag': 'DT', 'dep': 'det', 'shape': 'x', 'is_alpha': True, 'is_stop': True}\n",
      "{'text': 'modern', 'lemma': 'modern', 'POS': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'shape': 'xxxx', 'is_alpha': True, 'is_stop': False}\n",
      "{'text': 'chip', 'lemma': 'chip', 'POS': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'shape': 'xxxx', 'is_alpha': True, 'is_stop': False}\n",
      "{'text': 'building', 'lemma': 'building', 'POS': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'shape': 'xxxx', 'is_alpha': True, 'is_stop': False}\n",
      "{'text': 'factory', 'lemma': 'factory', 'POS': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'shape': 'xxxx', 'is_alpha': True, 'is_stop': False}\n",
      "{'text': 'intel', 'lemma': 'intel', 'POS': 'PROPN', 'tag': 'NNP', 'dep': 'nsubj', 'shape': 'xxxx', 'is_alpha': True, 'is_stop': False}\n",
      "{'text': 'would', 'lemma': 'would', 'POS': 'AUX', 'tag': 'MD', 'dep': 'aux', 'shape': 'xxxx', 'is_alpha': True, 'is_stop': True}\n",
      "{'text': 'need', 'lemma': 'need', 'POS': 'VERB', 'tag': 'VB', 'dep': 'advcl', 'shape': 'xxxx', 'is_alpha': True, 'is_stop': False}\n",
      "{'text': 'to', 'lemma': 'to', 'POS': 'PART', 'tag': 'TO', 'dep': 'aux', 'shape': 'xx', 'is_alpha': True, 'is_stop': True}\n",
      "{'text': 'invest', 'lemma': 'invest', 'POS': 'VERB', 'tag': 'VB', 'dep': 'xcomp', 'shape': 'xxxx', 'is_alpha': True, 'is_stop': False}\n",
      "{'text': '10x', 'lemma': '10x', 'POS': 'NOUN', 'tag': 'NNS', 'dep': 'dobj', 'shape': 'ddx', 'is_alpha': False, 'is_stop': False}\n",
      "{'text': 'more', 'lemma': 'more', 'POS': 'ADJ', 'tag': 'JJR', 'dep': 'advmod', 'shape': 'xxxx', 'is_alpha': True, 'is_stop': True}\n",
      "{'text': 'they', 'lemma': 'they', 'POS': 'PRON', 'tag': 'PRP', 'dep': 'nsubj', 'shape': 'xxxx', 'is_alpha': True, 'is_stop': True}\n",
      "{'text': 'are', 'lemma': 'be', 'POS': 'AUX', 'tag': 'VBP', 'dep': 'aux', 'shape': 'xxx', 'is_alpha': True, 'is_stop': True}\n",
      "{'text': 'drowning', 'lemma': 'drown', 'POS': 'VERB', 'tag': 'VBG', 'dep': 'relcl', 'shape': 'xxxx', 'is_alpha': True, 'is_stop': False}\n",
      "{'text': 'under', 'lemma': 'under', 'POS': 'ADP', 'tag': 'IN', 'dep': 'prep', 'shape': 'xxxx', 'is_alpha': True, 'is_stop': True}\n",
      "{'text': 'money', 'lemma': 'money', 'POS': 'NOUN', 'tag': 'NN', 'dep': 'pobj', 'shape': 'xxxx', 'is_alpha': True, 'is_stop': False}\n",
      "{'text': 'these', 'lemma': 'these', 'POS': 'DET', 'tag': 'DT', 'dep': 'det', 'shape': 'xxxx', 'is_alpha': True, 'is_stop': True}\n",
      "{'text': 'days', 'lemma': 'day', 'POS': 'NOUN', 'tag': 'NNS', 'dep': 'npadvmod', 'shape': 'xxxx', 'is_alpha': True, 'is_stop': False}\n",
      "{'text': 'but', 'lemma': 'but', 'POS': 'CCONJ', 'tag': 'CC', 'dep': 'cc', 'shape': 'xxx', 'is_alpha': True, 'is_stop': True}\n",
      "{'text': 'it', 'lemma': 'it', 'POS': 'PRON', 'tag': 'PRP', 'dep': 'nsubj', 'shape': 'xx', 'is_alpha': True, 'is_stop': True}\n",
      "{'text': \"'s\", 'lemma': 'be', 'POS': 'VERB', 'tag': 'VBZ', 'dep': 'conj', 'shape': \"'x\", 'is_alpha': False, 'is_stop': True}\n",
      "{'text': 'easier', 'lemma': 'easy', 'POS': 'ADJ', 'tag': 'JJR', 'dep': 'acomp', 'shape': 'xxxx', 'is_alpha': True, 'is_stop': False}\n",
      "{'text': 'to', 'lemma': 'to', 'POS': 'PART', 'tag': 'TO', 'dep': 'aux', 'shape': 'xx', 'is_alpha': True, 'is_stop': True}\n",
      "{'text': 'keep', 'lemma': 'keep', 'POS': 'VERB', 'tag': 'VB', 'dep': 'xcomp', 'shape': 'xxxx', 'is_alpha': True, 'is_stop': True}\n",
      "{'text': 'supply', 'lemma': 'supply', 'POS': 'NOUN', 'tag': 'NN', 'dep': 'dobj', 'shape': 'xxxx', 'is_alpha': True, 'is_stop': False}\n",
      "{'text': 'low', 'lemma': 'low', 'POS': 'ADJ', 'tag': 'JJ', 'dep': 'oprd', 'shape': 'xxx', 'is_alpha': True, 'is_stop': False}\n",
      "{'text': 'for', 'lemma': 'for', 'POS': 'ADP', 'tag': 'IN', 'dep': 'prep', 'shape': 'xxx', 'is_alpha': True, 'is_stop': True}\n",
      "{'text': 'better', 'lemma': 'well', 'POS': 'ADJ', 'tag': 'JJR', 'dep': 'amod', 'shape': 'xxxx', 'is_alpha': True, 'is_stop': False}\n",
      "{'text': 'profit', 'lemma': 'profit', 'POS': 'NOUN', 'tag': 'NN', 'dep': 'pobj', 'shape': 'xxxx', 'is_alpha': True, 'is_stop': False}\n"
     ]
    }
   ],
   "source": [
    "texts = df['text']\n",
    "doc1 = pipeline(texts[2])\n",
    "for i, token in enumerate(doc1):\n",
    "    print({\"text\": token.text,\n",
    "            \"lemma\": token.lemma_,\n",
    "            \"POS\": token.pos_,\n",
    "            \"tag\": token.tag_,\n",
    "            \"dep\": token.dep_,\n",
    "            \"shape\": token.shape_,\n",
    "            \"is_alpha\": token.is_alpha,\n",
    "            \"is_stop\": token.is_stop})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a pipeline to process comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<function __main__.remove_stop(doc)>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@Language.component('remove_stop')\n",
    "def remove_stop(doc):\n",
    "    return [token.lemma_.lower().strip().replace(\"'\", '') for token in doc if not token.is_stop and 1 < len(token.lemma_) < 25]\n",
    "\n",
    "pipeline.add_pipe('remove_stop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m\n",
      "============================= Pipeline Overview =============================\u001B[0m\n",
      "\n",
      "#   Component         Assigns               Requires   Scores             Retokenizes\n",
      "-   ---------------   -------------------   --------   ----------------   -----------\n",
      "0   tok2vec           doc.tensor                                          False      \n",
      "                                                                                     \n",
      "1   tagger            token.tag                        tag_acc            False      \n",
      "                                                                                     \n",
      "2   parser            token.dep                        dep_uas            False      \n",
      "                      token.head                       dep_las                       \n",
      "                      token.is_sent_start              dep_las_per_type              \n",
      "                      doc.sents                        sents_p                       \n",
      "                                                       sents_r                       \n",
      "                                                       sents_f                       \n",
      "                                                                                     \n",
      "3   attribute_ruler                                                       False      \n",
      "                                                                                     \n",
      "4   lemmatizer        token.lemma                      lemma_acc          False      \n",
      "                                                                                     \n",
      "5   ner               doc.ents                         ents_f             False      \n",
      "                      token.ent_iob                    ents_p                        \n",
      "                      token.ent_type                   ents_r                        \n",
      "                                                       ents_per_type                 \n",
      "                                                                                     \n",
      "6   remove_stop                                                           False      \n",
      "\n",
      "✔ No problems found.\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n   'requires': [],\n   'scores': [],\n   'retokenizes': False},\n  'tagger': {'assigns': ['token.tag'],\n   'requires': [],\n   'scores': ['tag_acc'],\n   'retokenizes': False},\n  'parser': {'assigns': ['token.dep',\n    'token.head',\n    'token.is_sent_start',\n    'doc.sents'],\n   'requires': [],\n   'scores': ['dep_uas',\n    'dep_las',\n    'dep_las_per_type',\n    'sents_p',\n    'sents_r',\n    'sents_f'],\n   'retokenizes': False},\n  'attribute_ruler': {'assigns': [],\n   'requires': [],\n   'scores': [],\n   'retokenizes': False},\n  'lemmatizer': {'assigns': ['token.lemma'],\n   'requires': [],\n   'scores': ['lemma_acc'],\n   'retokenizes': False},\n  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n   'requires': [],\n   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n   'retokenizes': False},\n  'remove_stop': {'assigns': [],\n   'requires': [],\n   'scores': [],\n   'retokenizes': False}},\n 'problems': {'tok2vec': [],\n  'tagger': [],\n  'parser': [],\n  'attribute_ruler': [],\n  'lemmatizer': [],\n  'ner': [],\n  'remove_stop': []},\n 'attrs': {'doc.ents': {'assigns': ['ner'], 'requires': []},\n  'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n  'doc.sents': {'assigns': ['parser'], 'requires': []},\n  'token.dep': {'assigns': ['parser'], 'requires': []},\n  'token.head': {'assigns': ['parser'], 'requires': []},\n  'token.tag': {'assigns': ['tagger'], 'requires': []},\n  'token.ent_type': {'assigns': ['ner'], 'requires': []},\n  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n  'token.is_sent_start': {'assigns': ['parser'], 'requires': []}}}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.analyze_pipes(pretty=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process a random comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin text:   billion dollars is pathetic for a modern chip building factory intel would need to invest 10x more they are drowning under money these days but it's easier to keep supply low for better profit \n",
      "['billion', 'dollar', 'pathetic', 'modern', 'chip', 'building', 'factory', 'intel', 'need', 'invest', '10x', 'drown', 'money', 'day', 'easy', 'supply', 'low', 'well', 'profit']\n"
     ]
    }
   ],
   "source": [
    "print('origin text: ', texts[2])\n",
    "print(pipeline(texts[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process all the comments and save the processed comments to a txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Estimated completion of line 5:   0%           Elapsed time: 00:00, estimated remaining time: ?",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90eaeae5059a48fa8316605d70dfa0cd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(t):\n",
    "    tokens = pipeline(t)\n",
    "    return tokens\n",
    "\n",
    "texts = texts.apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/142486 [00:00<?, ?it/s]\u001B[A\n",
      "  0%|          | 1/142486 [02:29<5930:24:15, 149.84s/it]\u001B[A\n",
      "100%|██████████| 142486/142486 [02:30<00:00, 949.71it/s]\u001B[A\n"
     ]
    }
   ],
   "source": [
    "f = open('../data/transaction.txt', 'w')\n",
    "for tokens in tqdm(texts):\n",
    "    if len(tokens) < 2:\n",
    "        continue\n",
    "    f.write(','.join(tokens))\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}